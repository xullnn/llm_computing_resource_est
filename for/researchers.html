<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Compare model efficiency, estimate FLOPs, and plan compute budgets for LLM research. Physics-based calculator for ML researchers and engineers.">
  <meta name="keywords" content="LLM FLOPs calculation, model efficiency, compute budget, transformer math, research planning">
  <meta property="og:title" content="LLM Research & Efficiency Calculator">
  <meta property="og:description" content="Compare models by FLOPs, KV cache, and bandwidth. Plan experiments before burning compute.">
  <meta property="og:type" content="website">
  <meta property="og:url" content="https://llmrunnable.com/for/researchers">
  <title>LLM Research & Efficiency Calculator | LLM Resource Sizer</title>
  <link rel="stylesheet" href="../css/main.css">
</head>
<body>
  <header class="hero-landing">
    <div class="hero-landing__copy">
      <p class="pill">ğŸ”¬ For Researchers & Engineers</p>
      <h1>Compare models efficiently</h1>
      <p class="lede">Estimate FLOPs, KV cache, and bandwidth before running experiments. Compare dense vs MoE, quantization trade-offs, and scaling behaviorâ€”all with transparent transformer math.</p>
      <div class="cta-buttons">
        <a class="btn primary" href="/?mode=compare#calculator">Compare models â†’</a>
        <a class="btn ghost" href="/">Full calculator</a>
      </div>
    </div>
  </header>
  
  <section class="landing-section">
    <div class="section-header">
      <h2 class="section-title">Research-focused comparisons</h2>
      <p class="section-subhead">Pre-configured scenarios for ML research</p>
    </div>
    <div class="landing-grid">
      <a href="/?preset=llama-3-70b&mode=compare&prompt=8000&new=2000#calculator" class="card-landing">
        <h3>Long-context inference</h3>
        <p>Llama 70B with 8K+ context: FLOPs breakdown and KV cache pressure.</p>
      </a>
      <a href="/?preset=deepseek-v3&mode=compare&prompt=4000#calculator" class="card-landing">
        <h3>MoE efficiency (DeepSeek-V3)</h3>
        <p>671B total, 37B active: compare effective compute vs dense models.</p>
      </a>
      <a href="/?preset=mixtral-8x22b&mode=compare&prompt=6000#calculator" class="card-landing">
        <h3>Mixtral 8x22B sparse vs dense</h3>
        <p>How does MoE stack up against Llama 70B at similar active params?</p>
      </a>
      <a href="/?preset=qwen-2.5-72b&mode=compare&prompt=16000&new=1000#calculator" class="card-landing">
        <h3>Scaling context length</h3>
        <p>Qwen 72B: see how bandwidth grows with O(seqÂ²) attention.</p>
      </a>
      <a href="/?preset=phi-3.5-mini&mode=compare&prompt=3000#calculator" class="card-landing">
        <h3>Small model efficiency</h3>
        <p>Phi-3.5 (3.8B): FLOPs and memory for edge/mobile scenarios.</p>
      </a>
      <a href="/?preset=llama-3-8b&mode=compare&prompt=4000#calculator" class="card-landing">
        <h3>Quantization impact</h3>
        <p>Compare BF16 vs FP8 vs INT4 for memory and throughput.</p>
      </a>
    </div>
  </section>

  <section class="landing-section">
    <div class="section-header">
      <h2 class="section-title">Why researchers use this tool</h2>
    </div>
    <div class="landing-grid">
      <article class="card-landing">
        <h3>ğŸ“ Transparent math</h3>
        <p>See the transformer equations: prefill O(seqÂ²), decode, KV cache formulas.</p>
      </article>
      <article class="card-landing">
        <h3>âš–ï¸ Compare architectures</h3>
        <p>Dense vs MoE, different layer counts, hidden sizes, and heads.</p>
      </article>
      <article class="card-landing">
        <h3>ğŸ§® FLOPs breakdown</h3>
        <p>Separate prefill (attention-heavy) from decode (memory-bound) phases.</p>
      </article>
      <article class="card-landing">
        <h3>ğŸ’¾ KV cache analysis</h3>
        <p>See how cache size scales with layers, heads, and sequence length.</p>
      </article>
      <article class="card-landing">
        <h3>ğŸ”¬ Plan experiments</h3>
        <p>Estimate compute hours before burning GPU budget on sweeps.</p>
      </article>
      <article class="card-landing">
        <h3>ğŸ“Š Quantization study</h3>
        <p>Model INT8 and INT4 speedups with utilization assumptions.</p>
      </article>
    </div>
  </section>

  <section class="landing-section">
    <div class="section-header">
      <h2 class="section-title">Technical features</h2>
    </div>
    <div class="landing-grid">
      <article class="card-landing">
        <h3>MoE support</h3>
        <p>Separate total params from active params per token.</p>
      </article>
      <article class="card-landing">
        <h3>Custom overrides</h3>
        <p>Manually set layers, hidden size, heads for custom architectures.</p>
      </article>
      <article class="card-landing">
        <h3>Utilization factors</h3>
        <p>Account for real-world compute (0.4-0.5Ã—) and bandwidth (0.6Ã—) efficiency.</p>
      </article>
      <article class="card-landing">
        <h3>Precision options</h3>
        <p>BF16, FP16, FP8, INT8, INT4 for weights and KV cache independently.</p>
      </article>
      <article class="card-landing">
        <h3>TTFT estimation</h3>
        <p>Time-to-first-token based on prefill FLOPs and hardware throughput.</p>
      </article>
      <article class="card-landing">
        <h3>Bandwidth modeling</h3>
        <p>Conservative (weights streamed) vs optimistic (weights resident) modes.</p>
      </article>
    </div>
  </section>

  <section class="landing-section">
    <div class="section-header">
      <h2 class="section-title">Research use cases</h2>
    </div>
    <div class="landing-grid">
      <article class="card-landing">
        <h3>Model selection</h3>
        <p>Which architecture fits your compute budget and latency targets?</p>
      </article>
      <article class="card-landing">
        <h3>Scaling laws</h3>
        <p>Explore how doubling params or context affects FLOPs and memory.</p>
      </article>
      <article class="card-landing">
        <h3>Inference benchmarking</h3>
        <p>Predict TTFT and throughput before running real benchmarks.</p>
      </article>
      <article class="card-landing">
        <h3>Hardware planning</h3>
        <p>Size clusters for large-scale evaluation or fine-tuning inference.</p>
      </article>
      <article class="card-landing">
        <h3>Efficiency analysis</h3>
        <p>Compare FLOPs/param across dense, MoE, and distilled models.</p>
      </article>
      <article class="card-landing">
        <h3>Paper validation</h3>
        <p>Check if reported specs match theoretical requirements.</p>
      </article>
    </div>
  </section>

  <section class="landing-section">
    <div class="section-header">
      <h2 class="section-title">Model presets included</h2>
      <p class="section-subhead">16+ architectures with accurate parameters</p>
    </div>
    <div style="background: var(--panel); border: 1px solid var(--border); border-radius: 14px; padding: 18px; color: var(--muted); font-size: 14px;">
      <strong style="color: var(--text); display: block; margin-bottom: 8px;">Covered models:</strong>
      Llama 3, 3.1, 3.2 (1B-405B) Â· Qwen 2.5 (0.5B-72B) Â· DeepSeek V2, V3 Â· Mixtral 8x7B, 8x22B Â· 
      Phi-3, 3.5 (mini, medium) Â· Gemma 2 (2B-27B) Â· Mistral 7B, Nemo 12B Â· 
      Yi 1.5 (6B-34B) Â· GLM-4 9B Â· StableLM 2 Â· Command R+ Â· DBRX Â· OLMo Â· InternLM 2.5 Â· Llama Guard 3
      <br><br>
      <strong style="color: var(--text);">Plus:</strong> Custom architecture support for research models.
    </div>
  </section>

  <section class="landing-section">
    <div class="section-header">
      <h2 class="section-title">Assumptions & methodology</h2>
    </div>
    <div style="background: var(--panel); border: 1px solid var(--border); border-radius: 14px; padding: 18px; color: var(--muted); font-size: 14px; line-height: 1.7;">
      <ul style="margin: 0; padding-left: 20px;">
        <li><strong>Prefill FLOPs:</strong> 2 Ã— active_params Ã— prompt_tokens + 4 Ã— layers Ã— promptÂ² Ã— hidden_size (attention)</li>
        <li><strong>Decode FLOPs/token:</strong> 2 Ã— active_params + 4 Ã— layers Ã— avg_seq Ã— hidden_size</li>
        <li><strong>KV cache:</strong> batch Ã— seq Ã— layers Ã— hidden Ã— 2 Ã— bytes (K+V per layer)</li>
        <li><strong>Bandwidth:</strong> Conservative: weights/batch + KV per token; Optimistic: assumes weight reuse</li>
        <li><strong>Utilization:</strong> Compute ~0.4-0.5 (kernel efficiency), Bandwidth ~0.6 (real-world achievable)</li>
      </ul>
      <p style="margin: 12px 0 0; font-size: 13px;">All formulas are exposed in results. Adjust utilization based on your profiling data.</p>
    </div>
  </section>

  <section class="landing-section" style="text-align: center; padding: 32px 20px;">
    <h2 class="section-title">Ready to compare models?</h2>
    <p class="section-subhead" style="margin-bottom: 20px;">Physics-based. Transparent. Citation-ready.</p>
    <a class="btn primary" href="/?mode=compare#calculator" style="font-size: 16px; padding: 14px 24px;">Compare models â†’</a>
  </section>

  <footer style="max-width: 1240px; margin: 0 auto 40px; padding: 0 20px; text-align: center; color: var(--muted); font-size: 13px;">
    <p>
      <a href="/" style="color: var(--accent); text-decoration: none;">â† Back to main calculator</a>
      Â· Built for the research community
      Â· <a href="https://github.com" style="color: var(--muted); text-decoration: none;">Open source & contributions welcome</a>
    </p>
  </footer>
</body>
</html>

