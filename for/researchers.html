<!doctype html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Strategic research planning for LLM architecture efficiency and compute budgeting. Physics-based analysis for ML engineers.">
  <title>Strategic Research Planner | LLM Resource Tool</title>
  <link rel="stylesheet" href="../css/main.css">
</head>
<body>
  <nav class="global-nav">
    <a href="/" class="nav-brand">
      <span class="nav-icon">üßÆ</span>
      <span class="nav-logo">LLM Sizer</span>
    </a>
    <button class="mobile-menu-toggle" aria-label="Toggle menu">‚ò∞</button>
    <div class="nav-links" id="navLinks">
      <a href="/" class="nav-item" data-i18n="navModels">Models</a>
      <a href="/calculator.html" class="nav-item" data-i18n="navCalculator">Calculator</a>
      <a href="/hardware/" class="nav-item" data-i18n="navHardware">Hardware</a>
      <a href="/for/enterprise.html" class="nav-item active" data-i18n="navGuides">Guides</a>
      <a href="/about.html" class="nav-item" data-i18n="navAbout">About</a>
    </div>
    <div class="nav-actions">
      <label class="field compact language-switch" style="margin-bottom: 0;">
        <select id="langSelect">
          <option value="en">English</option>
          <option value="zh">‰∏≠Êñá</option>
        </select>
      </label>
    </div>
  </nav>

  <header class="hero" style="border-bottom: none;">
    <div class="header-content">
      <h1 class="hero-title">Strategic Research Planning</h1>
      <p class="lede">Analyze architectural efficiency and model scaling bottlenecks</p>
    </div>
  </header>
  
  <section class="landing-section">
    <div class="landing-grid">
      <a href="/?preset=llama-3-70b&mode=compare&prompt=8000&new=2000#calculator" class="card-landing">
        <h3>Long-context inference</h3>
        <p>Llama 70B with 8K+ context: FLOPs breakdown and KV cache pressure.</p>
      </a>
      <a href="/?preset=deepseek-v3&mode=compare&prompt=4000#calculator" class="card-landing">
        <h3>MoE efficiency (DeepSeek-V3)</h3>
        <p>671B total, 37B active: compare effective compute vs dense models.</p>
      </a>
      <a href="/?preset=mixtral-8x22b&mode=compare&prompt=6000#calculator" class="card-landing">
        <h3>Mixtral 8x22B sparse vs dense</h3>
        <p>How does MoE stack up against Llama 70B at similar active params?</p>
      </a>
      <a href="/?preset=qwen-2.5-72b&mode=compare&prompt=16000&new=1000#calculator" class="card-landing">
        <h3>Scaling context length</h3>
        <p>Qwen 72B: see how bandwidth grows with O(seq¬≤) attention.</p>
      </a>
      <a href="/?preset=phi-3.5-mini&mode=compare&prompt=3000#calculator" class="card-landing">
        <h3>Small model efficiency</h3>
        <p>Phi-3.5 (3.8B): FLOPs and memory for edge/mobile scenarios.</p>
      </a>
      <a href="/?preset=llama-3-8b&mode=compare&prompt=4000#calculator" class="card-landing">
        <h3>Quantization impact</h3>
        <p>Compare BF16 vs FP8 vs INT4 for memory and throughput.</p>
      </a>
    </div>
  </section>

  <footer style="max-width: 1240px; margin: 32px auto 40px; padding: 0 20px; text-align: center; color: var(--muted); font-size: 13px;">
    <a href="../" style="color: var(--accent); text-decoration: none; font-weight: 600;">‚Üê Back to models</a>
  </footer>

  <script src="../js/i18n.js"></script>
  <script src="../js/nav.js"></script>
</body>
</html>
