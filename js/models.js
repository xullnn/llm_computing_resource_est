/**
 * Flagship open-source model presets.
 * paramsB = total parameters (billions) that must reside in memory.
 * activeParamsB = parameters actually used per token for MoE (compute-side).
 * hiddenSize / layers are optional; heuristics will fill gaps if absent.
 */
let MODEL_PRESETS = [
  {
    id: "qwen3-235b-a22b-fp8",
    provider: "Qwen",
    name: "Qwen3-235B-A22B-Thinking FP8 (2507)",
    repo: "Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
    hfUrl: "https://huggingface.co/Qwen/Qwen3-235B-A22B-Thinking-2507-FP8",
    paramsB: 235, // total parameters
    activeParamsB: 22, // MoE: activated per token
    hiddenSize: 4096, // from config.json
    layers: 94,
    heads: 64, // Q heads; KV heads = 4 (GQA)
    weightPrecision: "fp8",
    kvPrecision: "fp8",
  },
  {
    id: "qwen3-next-80b",
    provider: "Qwen",
    name: "Qwen3-Next-80B-A3B",
    repo: "Qwen/Qwen3-Next-80B-A3B-Thinking",
    hfUrl: "https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Thinking",
    paramsB: 81,
    activeParamsB: 3, // MoE: ~3B active params per token (sparse activation)
    hiddenSize: 2048, // from config.json
    layers: 48,
    heads: 16,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "qwen3-32b",
    provider: "Qwen",
    name: "Qwen3-32B",
    repo: "Qwen/Qwen3-32B",
    hfUrl: "https://huggingface.co/Qwen/Qwen3-32B",
    paramsB: 32,
    activeParamsB: 32,
    hiddenSize: 5120, // from config.json
    layers: 64,
    heads: 64,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "qwen3-14b",
    provider: "Qwen",
    name: "Qwen3-14B",
    repo: "Qwen/Qwen3-14B",
    hfUrl: "https://huggingface.co/Qwen/Qwen3-14B",
    paramsB: 14,
    activeParamsB: 14,
    hiddenSize: 5120, // from config.json
    layers: 40,
    heads: 40,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "qwen3-8b",
    provider: "Qwen",
    name: "Qwen3-8B",
    repo: "Qwen/Qwen3-8B",
    hfUrl: "https://huggingface.co/Qwen/Qwen3-8B",
    paramsB: 8,
    activeParamsB: 8,
    hiddenSize: 4096, // from config.json
    layers: 36,
    heads: 32,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "deepseek-v3",
    provider: "DeepSeek",
    name: "DeepSeek-V3 (MoE)",
    repo: "deepseek-ai/DeepSeek-V3",
    hfUrl: "https://huggingface.co/deepseek-ai/DeepSeek-V3",
    paramsB: 671,
    activeParamsB: 37, // active per token
    hiddenSize: 7168,
    layers: 61,
    heads: 128,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "deepseek-r1",
    provider: "DeepSeek",
    name: "DeepSeek-R1 (MoE)",
    repo: "deepseek-ai/DeepSeek-R1",
    hfUrl: "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    paramsB: 671,
    activeParamsB: 37,
    hiddenSize: 7168,
    layers: 61,
    heads: 128,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "deepseek-v2-chat",
    provider: "DeepSeek",
    name: "DeepSeek-V2-Chat (MoE)",
    repo: "deepseek-ai/DeepSeek-V2-Chat",
    hfUrl: "https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat",
    paramsB: 236,
    activeParamsB: 38,
    hiddenSize: 5120,
    layers: 60,
    heads: 128,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "llama-3.1-70b",
    provider: "Meta",
    name: "Llama 3.1 70B",
    repo: "meta-llama/Llama-3.1-70B-Instruct",
    hfUrl: "https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct",
    paramsB: 70,
    activeParamsB: 70,
    hiddenSize: 8192,
    layers: 80,
    heads: 64,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "llama-3.1-8b",
    provider: "Meta",
    name: "Llama 3.1 8B",
    repo: "meta-llama/Llama-3.1-8B-Instruct",
    hfUrl: "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
    paramsB: 8,
    activeParamsB: 8,
    hiddenSize: 4096,
    layers: 32,
    heads: 32,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "mixtral-8x22b",
    provider: "Mistral",
    name: "Mixtral 8x22B (MoE)",
    repo: "mistralai/Mixtral-8x22B-v0.1",
    hfUrl: "https://huggingface.co/mistralai/Mixtral-8x22B-v0.1",
    paramsB: 141,
    activeParamsB: 44,
    hiddenSize: 6144,
    layers: 56,
    heads: 48,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "mixtral-8x7b",
    provider: "Mistral",
    name: "Mixtral 8x7B (MoE)",
    repo: "mistralai/Mixtral-8x7B-v0.1",
    hfUrl: "https://huggingface.co/mistralai/Mixtral-8x7B-v0.1",
    paramsB: 47,
    activeParamsB: 12,
    hiddenSize: 4096,
    layers: 32,
    heads: 32,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "gemma2-27b",
    provider: "Google",
    name: "Gemma 2 27B",
    repo: "google/gemma-2-27b",
    hfUrl: "https://huggingface.co/google/gemma-2-27b",
    paramsB: 27,
    activeParamsB: 27,
    hiddenSize: 4608,
    layers: 48,
    heads: 72,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "gemma2-9b",
    provider: "Google",
    name: "Gemma 2 9B",
    repo: "google/gemma-2-9b",
    hfUrl: "https://huggingface.co/google/gemma-2-9b",
    paramsB: 9,
    activeParamsB: 9,
    hiddenSize: 3584,
    layers: 28,
    heads: 32,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "dbrx",
    provider: "Databricks",
    name: "DBRX (MoE)",
    repo: "databricks/dbrx-base",
    hfUrl: "https://huggingface.co/databricks/dbrx-base",
    paramsB: 132,
    activeParamsB: 36,
    hiddenSize: 8192,
    layers: 64,
    heads: 64,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
  {
    id: "yi-1.5-34b",
    provider: "01.AI",
    name: "Yi-1.5 34B",
    repo: "01-ai/Yi-1.5-34B",
    hfUrl: "https://huggingface.co/01-ai/Yi-1.5-34B",
    paramsB: 34,
    activeParamsB: 34,
    hiddenSize: 7168,
    layers: 60,
    heads: 56,
    weightPrecision: "bf16",
    kvPrecision: "bf16",
  },
];
